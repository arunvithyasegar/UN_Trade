{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b5349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f93298",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Function to get news from NewsAPI (requires free API key)\n",
    "def get_news(api_key, query, page_size=100):\n",
    "    \"\"\"\n",
    "    Fetches news articles from NewsAPI.\n",
    "    You can get a free API key at https://newsapi.org/\n",
    "    \"\"\"\n",
    "    base_url = \"https://newsapi.org/v2/everything\"\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'pageSize': page_size,\n",
    "        'language': 'en',\n",
    "        'sortBy': 'publishedAt',\n",
    "        'apiKey': api_key\n",
    "    }\n",
    "    \n",
    "    # Make the request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Function to extract country mentions from headline/title\n",
    "def extract_country(text):\n",
    "    \"\"\"Extracts countries mentioned in text using regex patterns for common countries.\"\"\"\n",
    "    countries = [\n",
    "        \"USA\", \"United States\", \"America\", \"China\", \"Japan\", \"South Korea\", \n",
    "        \"Taiwan\", \"Germany\", \"UK\", \"United Kingdom\", \"France\", \"India\", \n",
    "        \"Singapore\", \"Malaysia\", \"Vietnam\", \"Thailand\", \"Philippines\",\n",
    "        \"Russia\", \"Brazil\", \"Canada\", \"Mexico\", \"Italy\", \"Spain\"\n",
    "    ]\n",
    "    \n",
    "    # Create regex pattern with word boundaries\n",
    "    pattern = r'\\b(' + '|'.join(countries) + r')\\b'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    return ', '.join(matches) if matches else \"None detected\"\n",
    "\n",
    "# Function to perform sentiment analysis using VADER\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes sentiment of text and returns classification.\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    \n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif compound_score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Function to process news data\n",
    "def process_news_data(news_data, keywords):\n",
    "    \"\"\"\n",
    "    Processes news data to filter by keywords and extract relevant information.\n",
    "    \"\"\"\n",
    "    articles = news_data.get('articles', [])\n",
    "    \n",
    "    # Filter for relevant articles containing keywords\n",
    "    filtered_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.get('title', '').lower()\n",
    "        description = article.get('description', '').lower() if article.get('description') else ''\n",
    "        \n",
    "        if any(keyword.lower() in title or keyword.lower() in description for keyword in keywords):\n",
    "            filtered_articles.append(article)\n",
    "    \n",
    "    # Extract relevant details\n",
    "    processed_data = []\n",
    "    for article in filtered_articles[:20]:  # Limit to 20 articles\n",
    "        title = article.get('title', '')\n",
    "        url = article.get('url', '')\n",
    "        published_at = article.get('publishedAt', '')\n",
    "        \n",
    "        # Format timestamp\n",
    "        if published_at:\n",
    "            try:\n",
    "                dt = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                timestamp = dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            except:\n",
    "                timestamp = published_at\n",
    "        else:\n",
    "            timestamp = \"Unknown\"\n",
    "        \n",
    "        # Extract country\n",
    "        content = title + \" \" + article.get('description', '')\n",
    "        country = extract_country(content)\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment = analyze_sentiment(title)\n",
    "        \n",
    "        processed_data.append({\n",
    "            'Title': title,\n",
    "            'URL': url,\n",
    "            'Timestamp': timestamp,\n",
    "            'Country': country,\n",
    "            'Sentiment': sentiment\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Function to visualize sentiment distribution\n",
    "def visualize_sentiment(data):\n",
    "    \"\"\"Creates and saves a bar chart of sentiment distribution.\"\"\"\n",
    "    sentiment_counts = {\n",
    "        'Positive': sum(1 for item in data if item['Sentiment'] == 'Positive'),\n",
    "        'Neutral': sum(1 for item in data if item['Sentiment'] == 'Neutral'),\n",
    "        'Negative': sum(1 for item in data if item['Sentiment'] == 'Negative')\n",
    "    }\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['green', 'gray', 'red']\n",
    "    plt.bar(sentiment_counts.keys(), sentiment_counts.values(), color=colors)\n",
    "    \n",
    "    plt.title('Sentiment Distribution of News Headlines', fontsize=16)\n",
    "    plt.xlabel('Sentiment', fontsize=14)\n",
    "    plt.ylabel('Number of Headlines', fontsize=14)\n",
    "    \n",
    "    # Add count labels on top of each bar\n",
    "    for i, (sentiment, count) in enumerate(sentiment_counts.items()):\n",
    "        plt.text(i, count + 0.5, str(count), ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return sentiment_counts\n",
    "\n",
    "# Alternative function to get news from RSS feeds if you don't want to use NewsAPI\n",
    "def get_news_from_rss():\n",
    "    \"\"\"Fetches news from tech and business RSS feeds.\"\"\"\n",
    "    import feedparser\n",
    "    \n",
    "    # List of RSS feeds related to tech and business\n",
    "    rss_feeds = [\n",
    "        'https://www.theverge.com/rss/index.xml',\n",
    "        'https://feeds.feedburner.com/IeeeSpectrum',\n",
    "        'https://www.wired.com/feed/rss',\n",
    "        'https://www.eettaiwan.com/rss/',\n",
    "        'https://www.electronicsweekly.com/feed/'\n",
    "    ]\n",
    "    \n",
    "    all_entries = []\n",
    "    \n",
    "    for feed_url in rss_feeds:\n",
    "        try:\n",
    "            feed = feedparser.parse(feed_url)\n",
    "            entries = feed.entries\n",
    "            \n",
    "            for entry in entries:\n",
    "                article = {\n",
    "                    'title': entry.get('title', ''),\n",
    "                    'description': entry.get('summary', ''),\n",
    "                    'url': entry.get('link', ''),\n",
    "                    'publishedAt': entry.get('published', '')\n",
    "                }\n",
    "                all_entries.append(article)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {feed_url}: {e}\")\n",
    "    \n",
    "    return {'articles': all_entries}\n",
    "\n",
    "def main():\n",
    "    # Define keywords related to electronics, semiconductors, or manufacturing\n",
    "    keywords = [\n",
    "        'electronics', 'semiconductor', 'manufacturing', 'chip', 'processor',\n",
    "        'TSMC', 'Intel', 'AMD', 'Nvidia', 'Samsung', 'factory', 'production',\n",
    "        'supply chain', 'silicon', 'microchip', 'circuit', 'foundry'\n",
    "    ]\n",
    "    \n",
    "    # Choose your data source:\n",
    "    \n",
    "    # OPTION 1: NewsAPI (requires API key)\n",
    "    # Replace 'YOUR_API_KEY' with your actual NewsAPI key\n",
    "    # api_key = 'YOUR_API_KEY'\n",
    "    # query = ' OR '.join(keywords)\n",
    "    # news_data = get_news(api_key, query)\n",
    "    \n",
    "    # OPTION 2: RSS feeds (no API key required)\n",
    "    news_data = get_news_from_rss()\n",
    "    \n",
    "    if not news_data:\n",
    "        print(\"Failed to retrieve news data\")\n",
    "        return\n",
    "    \n",
    "    # Process the news data\n",
    "    processed_data = process_news_data(news_data, keywords)\n",
    "    \n",
    "    # Create DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('electronics_news_data.csv', index=False)\n",
    "    print(f\"Saved {len(df)} news articles to electronics_news_data.csv\")\n",
    "    \n",
    "    # Visualize sentiment\n",
    "    sentiment_counts = visualize_sentiment(processed_data)\n",
    "    print(\"Sentiment distribution:\", sentiment_counts)\n",
    "    \n",
    "    # Display the headlines and their sentiments\n",
    "    for i, item in enumerate(processed_data, 1):\n",
    "        print(f\"\\n{i}. {item['Title']}\")\n",
    "        print(f\"   Sentiment: {item['Sentiment']}\")\n",
    "        print(f\"   Country: {item['Country']}\")\n",
    "        print(f\"   Published: {item['Timestamp']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84b219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
